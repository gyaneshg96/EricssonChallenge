# -*- coding: utf-8 -*-
"""Challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bJW7ERZ9Er8TZQWrCo0gnjbYI33PCZD9
"""


import numpy as np
import pandas as pd
import nltk
import re
nltk.download('stopwords')
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer

dir = ""
trainfile = dir + "train_file.csv"
validfile = dir + "test_file.csv"
train_data = pd.read_csv(trainfile,engine="python")
test_data  = pd.read_csv(validfile,engine="python")
train_data.loc[train_data["overall"] == 1.0].head(15)

languages = ["english","french","german","dutch","spanish"]
def negative_word(word):
  if word in ['not','no',"non","ne","nicht","kein"]:
    return True
  if 'n\'t' in word:
    return True
  if 'n\'' in word:
    return True
  return False

def detect_language(text):
  if type(text) == 'float':
    return "english"
  try:
    text = text.lower()
  except AttributeError:
    pass
  which = "english"
  maximum = 0
  for lang in languages:
    currset = set(stopwords.words(lang))
    count = 0
    for w in text:
      if w in currset:
        count = count + 1
    if count > maximum:
      maximum = count
      which = lang 
  return which

def removepunctuation(sent):
  try:
    sent = re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,]", "", sent)
  except TypeError:
    pass
  return sent

def remove_words(sent):
  finalsent = []
  lang = detect_language(sent)
  currset = set(stopwords.words(lang))
  for w in sent:
    if (w not in currset) or (negative_word(w)==True):
      finalsent.append(w)
  return finalsent

def process_sent(sent):
  try:
    sent = removepunctuation(sent)
    val = [w.lower() for w in sent.split()]
    return remove_words(val)
  except AttributeError:
    return []

process_sent("Really fun work environment with startup")

def negative_word(word):
  if word == 'not' or word == 'no':
    return True
  if 'n\'t' in word:
    return True
  return False

def country(string):
  try:
    words = string.split()
    if words[-1][0] == '(':
      return words[-1][1:-1]
    else:
      return "USA"
  except AttributeError:
      return ""
  
def state(string):
  try:
    words = string.split()
    if words[-1].upper() == words[-1]:
      return words[-1]
    else:
      return "OTHER"
  except AttributeError:
      return ""
  

state("New York, NY")
negative_word("coudnt")

def year(sent):
  try:
    sent = re.sub('[!?.\']','',sent)
    return sent.split()[2]
  except IndexError:
    return ""

total_data = [train_data, test_data]
total_dataset = []
for dataset in total_data:
  dataset["status"] = dataset["status"].map({"Current Employee ":0, "Former Employee ":1})
  dataset["summary_imp"] = dataset["summary"].apply(process_sent)
  dataset["positive_imp"] = dataset["positives"].apply(process_sent)
  dataset["negative_imp"] = dataset["negatives"].apply(process_sent)
  dataset["advice"] = dataset["advice_to_mgmt"].apply(process_sent)
  dataset["advice_len"] = dataset["advice"].apply(len)
  dataset["country"]   = dataset["location"].apply(country)
  dataset["state"]     = dataset["location"].apply(state)
  dataset["month"]     = dataset["date"].apply(lambda date: date.split()[0])
  dataset["year"]     = dataset["date"].apply(year)
  dataset["language"] = dataset["positives"].apply(detect_language)
  listofdata = []
  for a,part in dataset.groupby("Place"):
    for i in range(5):
      score = "score_"+str(i+1)
      score_avg = part[score].mean()
      score_std = part[score].std()
      score_null_count = part[score].isnull().sum()
      score_null_random_list = np.random.randint(score_avg - score_std, score_avg + score_std, size=score_null_count)
      score_bins = np.arange(-0.25,5.26,0.5)
      positions = np.digitize(score_null_random_list,score_bins)
      a = score_bins[positions]
      b = score_bins[positions-1]
      part[score][np.isnan(part[score])] = (a+b)/2
    part.loc[part['score_6'] == 0, 'score_6'] = 0
    part.loc[part['score_6'] == 1, 'score_6'] = 1
    part.loc[part['score_6'] > 1, 'score_6'] = 2
    part.drop
    listofdata.append(part)
  total_dataset.append(listofdata)

list(train_data["summary_imp"].apply(lambda text : ' '.join(text)))

vec1 = CountVectorizer(min_df = 1)
vec2 = TfidfVectorizer(min_df = 1)

#corpus of all data
corpus = []
feature_length = 100

for dataset in total_data:
  temp = dataset["summary_imp"].apply(lambda text : ' '.join(text))
  corpus = corpus + list(temp)
  temp = dataset["negative_imp"].apply(lambda text : ' '.join(text))
  corpus = corpus + list(temp)
  temp = dataset["positive_imp"].apply(lambda text : ' '.join(text))
  corpus = corpus + list(temp)
  temp = dataset["advice"].apply(lambda text : ' '.join(text))
  corpus = corpus + list(temp)

matrix1 = vec1.fit_transform(corpus)
matrix2 = vec2.fit_transform(corpus)

lsa1 = TruncatedSVD(feature_length)
lsa2 = TruncatedSVD(feature_length)
lsa1.fit(matrix1)
#features1 = Normalizer(copy=False).fit(features1)
lsa2.fit(matrix2)
#features2 = Normalizer(copy=False).fit_transform(features2)

def vectorize(text,vec,lsa):
  text = ' '.join(text)
  text = [text]
  temp = vec.transform(text)
  return lsa.transform(temp)

vectorize(["people","great","work"],vec2,lsa2)[0]



total_dataset[0][0][['job_title','overall']].groupby('job_title').agg(['mean', 'count']).sort_values(('overall', 'count'))

for dataset in total_dataset:
  for part in dataset:
    part["summary_imp_1"] = part["summary_imp"].apply(lambda x: vectorize(x,vec1,lsa1))
    part["summary_imp_2"] = part["summary_imp"].apply(lambda x: vectorize(x,vec2,lsa2))
    part["positive_imp_1"] =part["positive_imp"].apply(lambda x: vectorize(x,vec1,lsa1))
    part["positive_imp_2"] =part["positive_imp"].apply(lambda x: vectorize(x,vec2,lsa2))
    part["negative_imp_1"] =part["negative_imp"].apply(lambda x: vectorize(x,vec1,lsa1))
    part["negative_imp_2"] =part["negative_imp"].apply(lambda x: vectorize(x,vec2,lsa2))
    
    drop_elements = ['Id', 'Place', 'Location', 'job_title', 'summary','positives','negatives','advice_to_mgmt']
	  part = part.drop(drop_elements, axis = 1)
total_dataset[0][0]

