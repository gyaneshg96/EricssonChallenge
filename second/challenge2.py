# -*- coding: utf-8 -*-
"""Challenge2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OAvM5AYH9hUlVp_G-Uz8eMwxHo7RheOe
"""

import numpy as np
import pandas as pd
import random
import nltk
import re
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))

dir = ""
trainfile = dir + "train_file2.csv"
validfile = dir + "test_file2.csv"
train_data = pd.read_csv(trainfile,engine="python")
test_data  = pd.read_csv(validfile,engine="python")
train_data["MaterialType"].unique()

train_data.loc[train_data["MaterialType"] == "CR"].head(15)

def remove_accents(text):
    try:
        text = unicode(text, 'utf-8')
    except (TypeError, NameError): # unicode is a default on python 3 
        pass
    text = unicodedata.normalize('NFD', text)
    text = text.encode('ascii', 'ignore')
    text = text.decode("utf-8")
    return str(text)

def removepunctuation(sent):
  try:
    sent = re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,]", "", sent)
  except TypeError:
    pass
  return sent

def process_text(text):
  try:
    text = text.lower()
    text = removepunctuation(text)
    vals = text.split()
    return remove_words(vals)
  except AttributeError:
    return []
  
def contains_music(text):
  try:
    text = text.lower()
    text = removepunctuation(text)
    return "music" in text
  except TypeError:
    return False
  
contains_music("Ink Spots (muscal group)")

total_data = [train_data, test_data]
total_dataset = []
for dataset in total_data:
  dataset["PublicationYear"] = dataset["PublicationYear"].apply(lambda s: re.sub(r'[^\d]','',s))
  dataset["Title_imp"] = dataset["Title"].apply(process_text)
  dataset["Subjects_imp"] = dataset["Subjects"].apply(process_text)
  dataset["ContainsMusic"] = dataset.apply(lambda row: contains_music(row.Creator) or contains_music(row.Subjects) or contains_music(row.Title))

train_data["MaterialType"] = pd.factorize(train_data["MaterialType"])

train1 = pd.concat([train_data,pd.get_dummies(train_data["MaterialType"])],axis = 1)
#train1[["Creator","BOOK"]].groupby("Creator").mean()
vals = list(train_data["MaterialType"].unique())
train1[["Creator"]+vals].groupby("Creator",sort=False).mean()

